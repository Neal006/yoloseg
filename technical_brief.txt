# Technical Brief: DINOv2-Based Multi-Scale Semantic Segmentation for Industrial Defect Detection

**A Comprehensive Architectural Deep-Dive for Research Documentation**

---

## Executive Summary

This document provides an exhaustive technical description of a semantic segmentation architecture designed for multi-defect detection in industrial imaging contexts. The system combines a **frozen pre-trained DINOv2 Vision Transformer (ViT-B/14) encoder** with a **custom multi-scale convolutional neural network (CNN) decoder** utilizing hierarchical skip connections extracted from intermediate transformer blocks. The architecture addresses the fundamental challenge of leveraging powerful self-supervised visual representations for pixel-level dense prediction tasks while maintaining computational efficiency.

---

## Table of Contents

1. [Architectural Overview](#1-architectural-overview)
2. [DINOv2 Encoder: In-Depth Analysis](#2-dinov2-encoder-in-depth-analysis)
   - 2.1 [Vision Transformer Foundation](#21-vision-transformer-foundation)
   - 2.2 [Patch Embedding Layer](#22-patch-embedding-layer)
   - 2.3 [Positional Encoding](#23-positional-encoding)
   - 2.4 [Transformer Block Architecture](#24-transformer-block-architecture)
   - 2.5 [Multi-Scale Feature Extraction Strategy](#25-multi-scale-feature-extraction-strategy)
   - 2.6 [Self-Supervised Pre-training Methodology](#26-self-supervised-pre-training-methodology)
3. [Multi-Scale CNN Decoder Architecture](#3-multi-scale-cnn-decoder-architecture)
   - 3.1 [Decoder Design Philosophy](#31-decoder-design-philosophy)
   - 3.2 [Convolutional Block Design](#32-convolutional-block-design)
   - 3.3 [Decoder Stage Architecture](#33-decoder-stage-architecture)
   - 3.4 [Progressive Upsampling Pipeline](#34-progressive-upsampling-pipeline)
   - 3.5 [Skip Connection Integration](#35-skip-connection-integration)
4. [Loss Function Formulation](#4-loss-function-formulation)
   - 4.1 [Dice Loss](#41-dice-loss)
   - 4.2 [Focal Loss](#42-focal-loss)
   - 4.3 [Combined Objective Function](#43-combined-objective-function)
5. [Training Methodology](#5-training-methodology)
6. [Evaluation Metrics](#6-evaluation-metrics)
7. [Data Augmentation Strategy](#7-data-augmentation-strategy)
8. [Technical Specifications & Hyperparameters](#8-technical-specifications--hyperparameters)

---

## 1. Architectural Overview

The proposed architecture follows an **encoder-decoder paradigm** with the following key design principles:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           INPUT IMAGE (B, 3, 518, 518)                       │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                        DINOv2 ENCODER (ViT-B/14)                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │  Patch Embedding: (B, 3, 518, 518) → (B, 1369, 768)                     ││
│  │  [CLS] Token Prepended → (B, 1370, 768)                                  ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                      │                                       │
│  ┌───────────────┐    ┌───────────────┐    ┌───────────────┐                 │
│  │ Block 0-2     │    │ Block 3-6     │    │ Block 7-10    │    Block 11    │
│  │               │→   │               │→   │               │→   └────┐      │
│  └───────────────┘    └───────────────┘    └───────────────┘         │      │
│         │                    │                    │                  │      │
│         │ Hook @3            │ Hook @7            │ Hook @11         │      │
│         ▼                    ▼                    ▼                  ▼      │
│    shallow (768)        mid (768)            deep (768)        final (768)  │
│    (B,768,37,37)        (B,768,37,37)        (B,768,37,37)                   │
└─────────────────────────────────────────────────────────────────────────────┘
         │                     │                    │
         │                     │                    │
         │                     │                    ▼
         │                     │   ┌───────────────────────────────────────┐
         │                     │   │     DECODER STAGE 1                   │
         │                     │   │     768 → 256, Upsample 2×            │
         │                     │   │     (B, 256, 74, 74)                   │
         │                     │   └───────────────────────────────────────┘
         │                     │                    │
         │                     │                    ▼
         │                     │   ┌───────────────────────────────────────┐
         │                     └──▶│     DECODER STAGE 2 + Skip           │
         │                         │     256+768 → 128, Upsample 2×        │
         │                         │     (B, 128, 148, 148)                 │
         │                         └───────────────────────────────────────┘
         │                                          │
         │                                          ▼
         │                         ┌───────────────────────────────────────┐
         └────────────────────────▶│     DECODER STAGE 3 + Skip           │
                                   │     128+768 → 64, Upsample 2×         │
                                   │     (B, 64, 296, 296)                  │
                                   └───────────────────────────────────────┘
                                                    │
                                                    ▼
                                   ┌───────────────────────────────────────┐
                                   │     DECODER STAGE 4 (Refinement)      │
                                   │     64 → 64, Upsample 2×               │
                                   │     (B, 64, 592, 592)                  │
                                   └───────────────────────────────────────┘
                                                    │
                                                    ▼
                                   ┌───────────────────────────────────────┐
                                   │     SEGMENTATION HEAD                  │
                                   │     1×1 Conv: 64 → num_classes         │
                                   │     Bilinear Interpolation → (518,518) │
                                   └───────────────────────────────────────┘
                                                    │
                                                    ▼
                          OUTPUT: (B, num_classes, 518, 518)
```

### Key Architectural Decisions:

| Component | Choice | Rationale |
|-----------|--------|-----------|
| Encoder | DINOv2 ViT-B/14 | Self-supervised pre-training yields robust, generalizable features |
| Encoder Freezing | Configurable (default: frozen) | Prevents catastrophic forgetting, reduces trainable parameters |
| Skip Layers | [3, 7, 11] | Multi-scale features from shallow, mid, and deep transformer blocks |
| Decoder | Custom Multi-Scale CNN | Lightweight, efficient feature aggregation and upsampling |
| Normalization | GroupNorm | Batch-size independent, suitable for small batch training |
| Activation | GELU | Smooth gradients, consistent with transformer pre-training |

---

## 2. DINOv2 Encoder: In-Depth Analysis

### 2.1 Vision Transformer Foundation

DINOv2 (Self-DIstillation with NO labels v2) represents Meta AI's state-of-the-art self-supervised vision model. The encoder utilizes the **ViT-B/14** (Vision Transformer Base with 14×14 patch size) variant with the following specifications:

| Parameter | Value | Description |
|-----------|-------|-------------|
| **Model Variant** | ViT-B/14 | Base-sized architecture |
| **Embedding Dimension** | 768 | Hidden dimension throughout transformer |
| **Number of Heads** | 12 | Multi-head self-attention heads |
| **Number of Layers** | 12 | Transformer encoder blocks (indexed 0-11) |
| **Patch Size** | 14 × 14 | Non-overlapping image patches |
| **MLP Ratio** | 4 | Feed-forward network expansion factor |
| **Total Parameters** | ~86M | Full encoder parameter count |

### 2.2 Patch Embedding Layer

The patch embedding layer serves as the **tokenization mechanism** for converting continuous image data into discrete sequence elements suitable for transformer processing.

#### Mathematical Formulation:

Given an input image $\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$ where $H = W = 518$ pixels:

1. **Patch Extraction**: The image is divided into non-overlapping patches of size $P \times P = 14 \times 14$:

$$\mathbf{x}_p^{(i,j)} = \mathbf{I}[i \cdot P : (i+1) \cdot P, \; j \cdot P : (j+1) \cdot P, \; :] \in \mathbb{R}^{P \times P \times 3}$$

2. **Number of Patches**:
$$N = \left\lfloor \frac{H}{P} \right\rfloor \times \left\lfloor \frac{W}{P} \right\rfloor = \frac{518}{14} \times \frac{518}{14} = 37 \times 37 = 1369$$

3. **Linear Projection**: Each flattened patch $\mathbf{x}_p \in \mathbb{R}^{P^2 \cdot 3 = 588}$ is projected to the embedding dimension:

$$\mathbf{z}_0^{(i)} = \mathbf{x}_p^{(i)} \cdot \mathbf{E} + \mathbf{b}, \quad \mathbf{E} \in \mathbb{R}^{588 \times 768}, \quad \mathbf{b} \in \mathbb{R}^{768}$$

The resulting **patch embedding sequence** is: $\mathbf{Z}_0 = [\mathbf{z}_0^{(1)}, \mathbf{z}_0^{(2)}, \ldots, \mathbf{z}_0^{(1369)}] \in \mathbb{R}^{1369 \times 768}$

### 2.3 Positional Encoding

DINOv2 employs **learnable 2D positional embeddings** to inject spatial information:

$$\mathbf{P} \in \mathbb{R}^{(N+1) \times 768}$$

Where the additional position corresponds to the `[CLS]` token. These embeddings are learned during pre-training and capture the relative spatial relationships between patches.

#### Token Construction:

A learnable `[CLS]` (classification) token $\mathbf{z}_{cls} \in \mathbb{R}^{768}$ is prepended:

$$\mathbf{Z}_0' = [\mathbf{z}_{cls}; \mathbf{z}_0^{(1)}; \mathbf{z}_0^{(2)}; \ldots; \mathbf{z}_0^{(N)}] + \mathbf{P}$$

Final input sequence: $\mathbf{Z}_0' \in \mathbb{R}^{1370 \times 768}$

### 2.4 Transformer Block Architecture

Each of the 12 transformer blocks follows the standard architecture with **Pre-LayerNorm (Pre-LN)** configuration:

```
┌─────────────────────────────────────────────────────────────────┐
│                    TRANSFORMER BLOCK (Layer ℓ)                  │
│                                                                 │
│   Input: z_{ℓ-1} ∈ ℝ^{1370 × 768}                               |
│                                                                 │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │         MULTI-HEAD SELF-ATTENTION (MHSA)                 │  │
│   │                                                          │  │
│   │   z' = LayerNorm(z_{ℓ-1})                                │  │ 
│   │                                                          │  │
│   │   Q = z' · W_Q,  K = z' · W_K,  V = z' · W_V             │  │
│   │   where W_Q, W_K, W_V ∈ ℝ^{768 × 768}                    │  │
│   │                                                          │  │
│   │   Split into 12 heads: Q_h, K_h, V_h ∈ ℝ^{1370 × 64}     │  │
│   │                                                          │  │
│   │   Attention(Q_h, K_h, V_h) = softmax(Q_h · K_h^T / √64)·V_h││
│   │                                                          │  │
│   │   Concat all heads → Linear projection                   │  │
│   │                                                          │  │
│   │   Output: MHSA(z') ∈ ℝ^{1370 × 768}                      │  │
│   └──────────────────────────────────────────────────────────┘  │
│                          │                                      │
│                          ▼                                      │
│              z_attn = z_{ℓ-1} + MHSA(z')   (Residual Connection)│
│                          │                                      │
│                          ▼                                      │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │              FEED-FORWARD NETWORK (FFN)                   │ │
│   │                                                          │  │
│   │   z'' = LayerNorm(z_attn)                                │  │
│   │                                                          │  │
│   │   FFN(z'') = GELU(z'' · W_1 + b_1) · W_2 + b_2           │  │
│   │   where W_1 ∈ ℝ^{768 × 3072}, W_2 ∈ ℝ^{3072 × 768}      │  │
│   │   (Expansion ratio = 4)                                  │  │
│   │                                                          │  │
│   │   Output: FFN(z'') ∈ ℝ^{1370 × 768}                      │  │
│   └──────────────────────────────────────────────────────────┘  │
│                          │                                      │
│                          ▼                                      │
│              z_ℓ = z_attn + FFN(z'')   (Residual Connection)    │
│                                                                  │
│   Output: z_ℓ ∈ ℝ^{1370 × 768}                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### 2.4.1 Multi-Head Self-Attention (MHSA)

The self-attention mechanism computes pairwise relationships between all token positions:

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$

Where $d_k = 768 / 12 = 64$ is the per-head dimension.

**Multi-Head formulation**:
$$\text{MHSA}(\mathbf{z}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_{12})\mathbf{W}^O$$
$$\text{head}_h = \text{Attention}(\mathbf{z}\mathbf{W}^Q_h, \mathbf{z}\mathbf{W}^K_h, \mathbf{z}\mathbf{W}^V_h)$$

#### 2.4.2 Feed-Forward Network (FFN)

The position-wise FFN applies two linear transformations with GELU activation:

$$\text{FFN}(\mathbf{z}) = \text{GELU}(\mathbf{z}\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2$$

**GELU Activation** (Gaussian Error Linear Unit):
$$\text{GELU}(x) = x \cdot \Phi(x) \approx \frac{x}{2}\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right]\right)$$

#### 2.4.3 Per-Block Parameter Count

| Component | Shape | Parameters |
|-----------|-------|------------|
| LayerNorm (×2) | 768 × 2 | 3,072 |
| Q, K, V projections | 768 × 768 × 3 | 1,769,472 |
| Output projection | 768 × 768 | 589,824 |
| FFN W₁ + b₁ | 768 × 3072 + 3072 | 2,362,368 |
| FFN W₂ + b₂ | 3072 × 768 + 768 | 2,360,064 |
| **Total per block** | | ~7.08M |
| **Total (12 blocks)** | | ~85M |

### 2.5 Multi-Scale Feature Extraction Strategy

A critical innovation in this architecture is the extraction of **multi-scale features** from intermediate transformer blocks using **forward hooks**. This approach addresses the fundamental limitation of ViT architectures: the absence of hierarchical feature pyramids.

#### Hook Registration:

```python
skip_layers = [3, 7, 11]  # Shallow, Mid, Deep

for layer_idx in skip_layers:
    hook = self.model.blocks[layer_idx].register_forward_hook(
        lambda module, input, output: self.features[layer_idx] = output
    )
```

#### Feature Semantics at Each Layer:

| Layer Index | Feature Type | Semantic Content | Resolution |
|-------------|--------------|------------------|------------|
| **Layer 3** | Shallow | Low-level features: edges, textures, local patterns | 37 × 37 |
| **Layer 7** | Mid | Mid-level features: parts, object components, texture combinations | 37 × 37 |
| **Layer 11** | Deep | High-level features: semantic concepts, object categories, global context | 37 × 37 |

> [!IMPORTANT]
> Unlike CNNs where feature resolution decreases through the network, ViT maintains **constant spatial resolution** (37×37 for 518×518 input with 14×14 patches) across all layers. The **depth** of semantic abstraction increases, not the spatial pyramid.

#### Feature Reshaping for CNN Decoder:

The transformer output is in sequence format $(B, N, C)$. For the CNN decoder, features are reshaped to spatial format:

```python
def _reshape_features(features: torch.Tensor, h: int, w: int) -> torch.Tensor:
    B, N, C = features.shape  # (B, 1370, 768)
    
    # Remove [CLS] token (position 0)
    if N == h * w + 1:
        features = features[:, 1:, :]  # (B, 1369, 768)
    
    # Reshape to 2D spatial format
    return features.permute(0, 2, 1).reshape(B, C, h, w)  # (B, 768, 37, 37)
```

### 2.6 Self-Supervised Pre-training Methodology

DINOv2 is trained using a **self-distillation framework** combined with **Masked Image Modeling (MIM)**:

#### 2.6.1 Self-Distillation (DINO Framework)

The training setup involves:
- **Student Network**: Updated via gradient descent
- **Teacher Network**: Exponential moving average (EMA) of student weights

The objective maximizes agreement between student and teacher on different augmented views:

$$\mathcal{L}_{DINO} = -\sum_i p_t(\mathbf{x}_i) \log p_s(\mathbf{x}_i')$$

Where $p_t$ and $p_s$ are teacher and student probability distributions over a learned codebook.

#### 2.6.2 Masked Image Modeling (iBOT-style)

A portion of patches are masked, and the student learns to predict their representations:

$$\mathcal{L}_{MIM} = \sum_{i \in \mathcal{M}} \|f_s(\tilde{\mathbf{x}})_i - f_t(\mathbf{x})_i\|_2^2$$

Where $\mathcal{M}$ is the set of masked patch positions.

#### 2.6.3 Training Data

DINOv2 was trained on **LVD-142M**, a curated dataset of 142 million images, providing:
- Diverse visual concepts
- High-quality, properly curated images
- Automatic filtering of inappropriate content

---

## 3. Multi-Scale CNN Decoder Architecture

### 3.1 Decoder Design Philosophy

The decoder addresses several key challenges:

1. **Resolution Recovery**: Upsampling from 37×37 (patch grid) to 518×518 (original resolution)
2. **Multi-Scale Fusion**: Integrating features from multiple transformer depths
3. **Channel Reduction**: Transitioning from 768-dimensional transformer space to segmentation logits
4. **Computational Efficiency**: Minimizing trainable parameters while maintaining capacity

### 3.2 Convolutional Block Design

The fundamental building block uses a **1×1 → 3×3 convolutional pattern** with GroupNorm and GELU:

```
┌──────────────────────────────────────────────────────────────┐
│                      CONV BLOCK                               │
│                                                               │
│   Input: (B, in_channels, H, W)                               │
│                                                               │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ 1×1 Conv (Channel Projection)                          │ │
│   │   - Reduces/expands channels without spatial mixing    │ │
│   │   - No bias (followed by normalization)                │ │
│   └────────────────────────────────────────────────────────┘ │
│                           ↓                                   │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ GroupNorm (num_groups=32 or adaptive)                  │ │
│   │   - Batch-size independent normalization               │ │
│   │   - Groups: min(32, out_channels) where divisible      │ │
│   └────────────────────────────────────────────────────────┘ │
│                           ↓                                   │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ GELU Activation                                        │ │
│   │   - Smooth, non-monotonic activation                   │ │
│   └────────────────────────────────────────────────────────┘ │
│                           ↓                                   │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ 3×3 Conv (Spatial Processing)                          │ │
│   │   - padding=1 (same spatial resolution)                │ │
│   │   - No bias (followed by normalization)                │ │
│   └────────────────────────────────────────────────────────┘ │
│                           ↓                                   │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ GroupNorm                                              │ │
│   └────────────────────────────────────────────────────────┘ │
│                           ↓                                   │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ GELU Activation                                        │ │
│   └────────────────────────────────────────────────────────┘ │
│                           ↓                                   │
│   ┌────────────────────────────────────────────────────────┐ │
│   │ Dropout2D (p=0.1)                                      │ │
│   │   - Spatial dropout for regularization                 │ │
│   └────────────────────────────────────────────────────────┘ │
│                                                               │
│   Output: (B, out_channels, H, W)                             │
└──────────────────────────────────────────────────────────────┘
```

#### GroupNorm Rationale:

Unlike BatchNorm, GroupNorm divides channels into groups and normalizes within each group:

$$\text{GroupNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu_G}{\sqrt{\sigma_G^2 + \epsilon}} \cdot \gamma + \beta$$

Where $\mu_G$ and $\sigma_G^2$ are computed within each channel group. This is **independent of batch size**, making it suitable for:
- Small batch training (common with large models)
- Variable batch sizes during inference
- Gradient accumulation strategies

### 3.3 Decoder Stage Architecture

Each decoder stage performs:
1. **Skip Connection Fusion** (optional)
2. **Convolutional Processing**
3. **Bilinear Upsampling**

```python
class DecoderStage(nn.Module):
    def forward(self, x: torch.Tensor, skip: Optional[torch.Tensor] = None):
        # 1. Align skip connection resolution if needed
        if skip is not None:
            if skip.shape[2:] != x.shape[2:]:
                skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear')
            x = torch.cat([x, skip], dim=1)  # Channel concatenation
        
        # 2. Convolutional processing
        x = self.conv_block(x)
        
        # 3. Spatial upsampling
        if self.upsample_scale > 1:
            x = F.interpolate(x, scale_factor=2, mode='bilinear')
        
        return x
```

### 3.4 Progressive Upsampling Pipeline

The decoder performs **4-stage progressive upsampling** from 37×37 to 518×518:

| Stage | Input | Skip | Output Channels | Output Size | Upsample Factor |
|-------|-------|------|-----------------|-------------|-----------------|
| **Stage 1** | deep (768) | None | 256 | 74 × 74 | 2× |
| **Stage 2** | prev (256) | mid (768) | 128 | 148 × 148 | 2× |
| **Stage 3** | prev (128) | shallow (768) | 64 | 296 × 296 | 2× |
| **Stage 4** | prev (64) | None | 64 | 592 × 592 | 2× |
| **Head** | prev (64) | - | num_classes | 518 × 518 | Resize |

#### Spatial Resolution Progression:

$$37 \xrightarrow{\times 2} 74 \xrightarrow{\times 2} 148 \xrightarrow{\times 2} 296 \xrightarrow{\times 2} 592 \xrightarrow{\text{resize}} 518$$

### 3.5 Skip Connection Integration

Skip connections inject **transformer features from earlier layers** into the decoder:

```
                    Encoder                           Decoder
                    
   Layer 3 (shallow) ──────────────────────────────────┐
          │                                             │
          ▼                                             ▼
   Layer 7 (mid) ──────────────────────────────┐       Stage 3
          │                                     │       (128+768 → 64)
          ▼                                     ▼       
   Layer 11 (deep) ──────────────────────────▶ Stage 1  Stage 2
                                               (768→256) (256+768 → 128)
```

#### Skip Connection Benefits:

1. **Gradient Flow**: Shorter gradient paths from output to encoder
2. **Multi-Scale Information**: Combines local (shallow) with global (deep) features
3. **Detail Preservation**: Low-level features aid in boundary refinement

### 3.6 Segmentation Head

The final segmentation head is a simple **1×1 convolution** projecting to class logits:

$$\text{head}: \mathbb{R}^{B \times 64 \times H \times W} \rightarrow \mathbb{R}^{B \times C \times H \times W}$$

Where $C$ = number of classes (4 for this application: Background, Dust, Rundown, Scratch).

### 3.7 Weight Initialization

The decoder uses **Kaiming (He) initialization** optimized for GELU activation:

```python
def _init_weights(self):
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.GroupNorm):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
```

---

## 4. Loss Function Formulation

The training objective combines **Dice Loss** and **Focal Loss** to address both region overlap and class imbalance challenges inherent in defect detection.

### 4.1 Dice Loss

The Dice Loss directly optimizes the Sørensen-Dice coefficient, measuring set similarity:

#### Mathematical Definition:

$$\mathcal{L}_{Dice} = 1 - \frac{2 \sum_{i}^{N} p_i g_i + \epsilon}{\sum_{i}^{N} p_i + \sum_{i}^{N} g_i + \epsilon}$$

Where:
- $p_i$ = predicted probability for pixel $i$ (after softmax)
- $g_i$ = ground truth (one-hot encoded) for pixel $i$
- $\epsilon$ = smoothing factor (1e-6) for numerical stability

#### Multi-Class Extension:

For $C$ classes, Dice loss is computed per-class and averaged:

$$\mathcal{L}_{Dice} = 1 - \frac{1}{C} \sum_{c=1}^{C} \frac{2 \sum_{i} p_{i,c} \cdot g_{i,c} + \epsilon}{\sum_{i} p_{i,c} + \sum_{i} g_{i,c} + \epsilon}$$

#### Implementation Details:

```python
# Apply softmax to get probabilities
pred_prob = F.softmax(pred, dim=1)  # (B, C, H, W)

# One-hot encode target
target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()

# Compute Dice per class
dims = (0, 2, 3)  # Aggregate over batch and spatial dimensions
intersection = (pred_prob * target_one_hot).sum(dim=dims)
pred_sum = pred_prob.sum(dim=dims)
target_sum = target_one_hot.sum(dim=dims)

dice = (2.0 * intersection + smooth) / (pred_sum + target_sum + smooth)
loss = 1.0 - dice.mean()
```

### 4.2 Focal Loss

Focal Loss addresses **class imbalance** by down-weighting well-classified examples:

#### Mathematical Definition:

$$\mathcal{L}_{Focal} = -\alpha (1 - p_t)^\gamma \log(p_t)$$

Where:
- $p_t = p$ if $y = 1$, else $1 - p$
- $\alpha$ = class balancing factor (0.25)
- $\gamma$ = focusing parameter (2.0)

#### Effect of $\gamma$:

| $p_t$ | Standard CE | Focal (γ=2) |
|-------|-------------|-------------|
| 0.9 | 0.105 | 0.001 |
| 0.5 | 0.693 | 0.173 |
| 0.1 | 2.303 | 1.864 |

> [!TIP]
> The **focusing parameter** γ=2 causes well-classified examples (high $p_t$) to contribute much less to the loss, allowing the model to focus on hard, misclassified examples.

#### Class Weights:

Per-class weights account for defect frequency imbalance:

```yaml
class_weights: [0.3, 3.0, 1.5, 1.5]  # [Background, Dust, Rundown, Scratch]
```

The **Dust class** receives 10× weight relative to background due to its rarity.

### 4.3 Combined Objective Function

The final training objective is a weighted sum:

$$\mathcal{L}_{Total} = \lambda_{Dice} \cdot \mathcal{L}_{Dice} + \lambda_{Focal} \cdot \mathcal{L}_{Focal}$$

With default weights:
- $\lambda_{Dice}$ = 0.5
- $\lambda_{Focal}$ = 0.5

#### Synergistic Effects:

| Loss | Strength | Addresses |
|------|----------|-----------|
| Dice | Region overlap, smooth boundaries | Imbalanced foreground/background |
| Focal | Hard example mining | Class imbalance, boundary pixels |

---

## 5. Training Methodology

### 5.1 Optimizer Configuration

**AdamW** optimizer with decoupled weight decay:

```python
optimizer = torch.optim.AdamW(
    model.get_trainable_params(),
    lr=1e-5,
    weight_decay=0.01
)
```

#### AdamW Update Rule:

$$\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)$$

Where weight decay $\lambda$ is applied **directly to parameters**, not gradients.

### 5.2 Learning Rate Schedule

**Cosine Annealing with Warm Restarts** (SGDR):

$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_i}\pi\right)\right)$$

Configuration:
- $T_0$ = num_epochs / 3 ≈ 66 epochs (initial restart period)
- $T_{mult}$ = 2 (period doubling after each restart)
- $\eta_{min}$ = 1e-6

### 5.3 Mixed Precision Training

Automatic Mixed Precision (AMP) using PyTorch's `GradScaler`:

```python
with autocast(enabled=True):
    outputs = model(images)
    loss = criterion(outputs, masks)

scaler.scale(loss).backward()
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
scaler.step(optimizer)
scaler.update()
```

**Benefits**:
- ~50% memory reduction
- ~2× training speedup
- Maintained numerical stability via loss scaling

### 5.4 Gradient Accumulation

For effective larger batch sizes without memory increase:

```python
accumulation_steps = 1  # Configurable
loss = loss / accumulation_steps

if (batch_idx + 1) % accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()
```

**Effective batch size** = `batch_size × accumulation_steps`

### 5.5 Early Stopping

Prevents overfitting by monitoring validation IoU:

```python
class EarlyStopping:
    patience: int = 5
    min_delta: float = 0.001
    
    def __call__(self, score: float) -> bool:
        if score < self.best_score + self.min_delta:
            self.counter += 1
            return self.counter >= self.patience
        else:
            self.best_score = score
            self.counter = 0
            return False
```

---

## 6. Evaluation Metrics

### 6.1 Intersection over Union (IoU / Jaccard Index)

$$\text{IoU}_c = \frac{|P_c \cap G_c|}{|P_c \cup G_c|} = \frac{TP_c}{TP_c + FP_c + FN_c}$$

**Mean IoU (mIoU)**: Average across all valid classes.

### 6.2 Dice Coefficient (F1 Score)

$$\text{Dice}_c = \frac{2|P_c \cap G_c|}{|P_c| + |G_c|} = \frac{2 \cdot TP_c}{2 \cdot TP_c + FP_c + FN_c}$$

### 6.3 Pixel Accuracy

$$\text{Acc} = \frac{\sum_c TP_c}{\sum_i \mathbb{1}[g_i \neq \text{ignore}]}$$

### 6.4 Boundary-Tolerant Metrics

For industrial defect detection where annotation precision varies:

#### Boundary-Tolerant IoU:
Uses morphological dilation to allow pixel-level tolerance at boundaries.

#### Instance Detection Rate:
Evaluates object-level detection (did we find the defect?) rather than pixel-level accuracy.

#### Boundary Accuracy:
Measures average distance from predicted boundaries to ground truth boundaries.

---

## 7. Data Augmentation Strategy

Augmentation pipeline using **Albumentations**:

| Augmentation | Probability | Parameters |
|--------------|-------------|------------|
| Horizontal Flip | 0.5 | - |
| Vertical Flip | 0.3 | - |
| Rotation | 0.5 | ±15° |
| Brightness/Contrast | 0.5 | ±20% |
| Gaussian Blur | 0.3 | kernel 3-7 |
| Coarse Dropout | 0.3 | 1-8 holes |
| Elastic Transform | 0.3 | Default |
| CLAHE | 0.3 | clip_limit=2.0 |

### ImageNet Normalization:

```python
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]
```

Applied to match DINOv2's pre-training distribution.

---

## 8. Technical Specifications & Hyperparameters

### 8.1 Model Summary

| Component | Parameters | Trainable |
|-----------|------------|-----------|
| DINOv2 Encoder (ViT-B/14) | ~86M | 0 (frozen) / 86M (unfrozen) |
| Multi-Scale Decoder | ~2.5M | 2.5M |
| **Total** | ~88.5M | ~2.5M (encoder frozen) |

### 8.2 Default Configuration

```yaml
model:
  encoder: "dinov2_vitb14"
  encoder_frozen: false
  skip_layers: [3, 7, 11]
  decoder_channels: [256, 128, 64]
  num_classes: 4  # Background + 3 defect classes

training:
  batch_size: 8
  num_epochs: 200
  learning_rate: 0.00001
  weight_decay: 0.01
  warmup_epochs: 20
  gradient_clip: 1.0

loss:
  dice_weight: 0.5
  focal_weight: 0.5
  focal_alpha: 0.25
  focal_gamma: 2.0
  class_weights: [0.3, 3.0, 1.5, 1.5]

data:
  image_size: 518
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
```

### 8.3 Memory Requirements

| Configuration | GPU Memory | Batch Size |
|---------------|------------|------------|
| Encoder Frozen | ~8 GB | 8 |
| Encoder Unfrozen | ~16 GB | 4 |
| With Gradient Checkpointing | ~12 GB | 8 |

---

## 9. Theoretical Analysis

### 9.1 Why DINOv2 for Segmentation?

1. **Self-Supervised Pre-training**: Features learned without task-specific labels generalize better than supervised ImageNet features
2. **Dense Feature Quality**: Unlike contrastive methods (CLIP), DINOv2's distillation objective preserves local patch information
3. **Semantic Consistency**: Features maintain semantic meaning across views, crucial for segmentation

### 9.2 Skip Layer Selection Rationale

The choice of layers [3, 7, 11] follows a **roughly uniform depth spacing**:

- **Layer 3 (25%)**: Early features with local, textural information
- **Layer 7 (58%)**: Mid-level features with part-level representations  
- **Layer 11 (92%)**: Near-final features with high-level semantics

### 9.3 Resolution Considerations

The 518×518 input size is chosen because:
$$518 = 14 \times 37$$

This ensures **exact divisibility** by the patch size, avoiding interpolation artifacts.

---

## 10. References

1. Oquab, M., et al. (2023). **DINOv2: Learning Robust Visual Features without Supervision**. arXiv:2304.07193
2. Dosovitskiy, A., et al. (2020). **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**. ICLR 2021
3. Caron, M., et al. (2021). **Emerging Properties in Self-Supervised Vision Transformers (DINO)**. ICCV 2021
4. Lin, T.Y., et al. (2017). **Focal Loss for Dense Object Detection**. ICCV 2017
5. Sudre, C.H., et al. (2017). **Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations**. MICCAI DL Workshop
6. Ronneberger, O., et al. (2015). **U-Net: Convolutional Networks for Biomedical Image Segmentation**. MICCAI 2015

---

*Document Version: 1.0 | Generated: 2026-02-05*
